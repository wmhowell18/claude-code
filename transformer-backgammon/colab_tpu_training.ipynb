{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# Transformer Backgammon - Colab Training (TPU / GPU / CPU)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wmhowell18/claude-code/blob/main/transformer-backgammon/colab_tpu_training.ipynb)\n\nTrain a transformer-based backgammon AI on Google Colab.\n\n**For best performance:** Runtime ‚Üí Change runtime type ‚Üí **TPU** (bfloat16 mixed precision enabled automatically)\n\nWorks on GPU and CPU too ‚Äî just slower."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": "## 1. Install Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pip_install"
   },
   "outputs": [],
   "source": "import jax\n\nbackend = jax.default_backend()\nprint(f\"Detected backend: {backend}\")\n\nif backend == \"tpu\":\n    # TPU runtime ‚Äî install TPU-specific JAX wheels\n    !pip install -q \"jax[tpu]>=0.4.26\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\nelse:\n    # GPU or CPU ‚Äî standard JAX is fine\n    !pip install -q \"jax>=0.4.26\" \"jaxlib>=0.4.26\"\n\n# Install the backgammon package from the repo\n!pip install -q \"flax>=0.8.0\" \"optax>=0.1.7\"\n!pip install -q \"git+https://github.com/wmhowell18/claude-code.git@main#subdirectory=transformer-backgammon\"\n\n# Verify the package installed correctly\ntry:\n    import backgammon\n    print(f\"\\n‚úÖ Installed! Backend: {jax.default_backend()}, Devices: {jax.device_count()}\")\nexcept ImportError:\n    print(\"\\n‚ùå Install failed ‚Äî try restarting runtime (Runtime > Restart runtime) and re-running this cell\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": "## 2. Verify Hardware & bfloat16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_tpu"
   },
   "outputs": [],
   "source": "import jax\nimport jax.numpy as jnp\nimport time\n\nbackend = jax.default_backend()\ndevices = jax.devices()\nprint(f\"JAX version:  {jax.__version__}\")\nprint(f\"Backend:      {backend}\")\nprint(f\"Device count: {len(devices)}\")\nfor d in devices:\n    print(f\"  {d}\")\n\n# Determine if bfloat16 is beneficial on this hardware\nUSE_BFLOAT16 = backend in (\"tpu\", \"gpu\")\n\n# bfloat16 matmul benchmark\nx32 = jnp.ones((1024, 1024), dtype=jnp.float32)\nx16 = jnp.ones((1024, 1024), dtype=jnp.bfloat16)\n\n# Warmup\njnp.dot(x32, x32).block_until_ready()\njnp.dot(x16, x16).block_until_ready()\n\nt0 = time.time()\nfor _ in range(100):\n    jnp.dot(x32, x32).block_until_ready()\nt32 = time.time() - t0\n\nt0 = time.time()\nfor _ in range(100):\n    jnp.dot(x16, x16).block_until_ready()\nt16 = time.time() - t0\n\nspeedup = t32 / t16\nprint(f\"\\nfloat32 matmul:  {t32*10:.1f} ms\")\nprint(f\"bfloat16 matmul: {t16*10:.1f} ms\")\nprint(f\"Speedup:         {speedup:.2f}x\")\nprint(f\"\\n‚úÖ Will use: {'bfloat16' if USE_BFLOAT16 else 'float32'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drive"
   },
   "source": [
    "## 3. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for this training run\n",
    "SAVE_DIR = Path('/content/drive/MyDrive/backgammon_training')\n",
    "CHECKPOINT_DIR = SAVE_DIR / 'checkpoints'\n",
    "LOG_DIR = SAVE_DIR / 'logs'\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"‚úÖ Logs: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": "## 4. Configure Training\n\nTwo presets:\n- **Quick** (~2,500 games, ~15-30 min on TPU): validates the pipeline works\n- **Full** (~16,000 games, ~4-8 hrs on TPU): real training run"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": "#@title Training preset { run: \"auto\" }\nPRESET = \"quick\"  #@param [\"quick\", \"full\"]\n\nfrom backgammon.training.train import TrainingConfig, v6e_quick_training_config\n\nif PRESET == \"quick\":\n    # Quick validation run ‚Äî uses v6e-optimized config\n    # ~2,500 games, small model (~500K params), bfloat16\n    config = v6e_quick_training_config()\nelse:\n    # Full training run ‚Äî larger model, more games\n    config = TrainingConfig(\n        warmstart_games=1000,\n        early_phase_games=5000,\n        mid_phase_games=5000,\n        late_phase_games=5000,\n        embed_dim=128,\n        num_heads=8,\n        num_layers=4,\n        ff_dim=512,\n        games_per_batch=50,\n        training_batch_size=512,\n        train_steps_per_game_batch=10,\n        train_policy=False,\n        compute_dtype='bfloat16' if USE_BFLOAT16 else None,\n        replay_buffer_size=100_000,\n        replay_buffer_min_size=1000,\n        checkpoint_every_n_batches=100,\n        log_every_n_batches=10,\n        eval_every_n_batches=50,\n        eval_num_games=50,\n        seed=42,\n    )\n\n# Override dtype based on hardware detection\nif not USE_BFLOAT16:\n    config.compute_dtype = None\n\n# Point checkpoints/logs to Google Drive\nconfig.checkpoint_dir = str(CHECKPOINT_DIR)\nconfig.log_dir = str(LOG_DIR)\n\ntotal_games = (config.warmstart_games + config.early_phase_games +\n               config.mid_phase_games + config.late_phase_games)\n\nprint(f\"Preset:       {PRESET}\")\nprint(f\"Total games:  {total_games:,}\")\nprint(f\"Model:        {config.num_layers}L / {config.embed_dim}d / {config.num_heads}H / {config.ff_dim}ff\")\nprint(f\"Batch size:   {config.training_batch_size}\")\nprint(f\"Dtype:        {'bfloat16' if config.compute_dtype else 'float32'}\")\nprint(f\"Checkpoints:  {config.checkpoint_dir}\")\nprint(f\"\\n‚úÖ Ready to train!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": "## 5. Run Training\n\n| Preset | Games | TPU time | GPU time | CPU time |\n|--------|-------|----------|----------|----------|\n| Quick  | 2,500 | ~15-30 min | ~1 hr | ~3 hrs |\n| Full   | 16,000 | ~4-8 hrs | ~12+ hrs | not recommended |\n\nCheckpoints are saved to Google Drive, so you won't lose progress if disconnected."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": "from backgammon.training.train import train\nimport time\n\nprint(f\"Starting {PRESET} training run...\")\nprint(f\"  dtype: {'bfloat16' if config.compute_dtype else 'float32'}\")\nprint(f\"  backend: {jax.default_backend()}\")\nprint()\n\nt0 = time.time()\ntry:\n    train(config)\n    elapsed = time.time() - t0\n    print(f\"\\nüéâ Training complete! ({elapsed/60:.1f} minutes)\")\nexcept KeyboardInterrupt:\n    elapsed = time.time() - t0\n    print(f\"\\n‚ö†Ô∏è Interrupted after {elapsed/60:.1f} min. Checkpoints saved to Google Drive.\")\nexcept Exception as e:\n    elapsed = time.time() - t0\n    print(f\"\\n‚ùå Error after {elapsed/60:.1f} min: {e}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## 6. Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_logs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "log_file = LOG_DIR / 'training_log.jsonl'\n",
    "\n",
    "if log_file.exists():\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"Total entries: {len(lines)}\\n\")\n",
    "    print(\"Last 20 batches:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for line in lines[-20:]:\n",
    "        e = json.loads(line)\n",
    "        phase = e.get('phase', 'unknown')\n",
    "        batch = e.get('batch_num', 0)\n",
    "        loss = e.get('loss', 0.0)\n",
    "        games = e.get('total_games', 0)\n",
    "        print(f\"[{phase:8s}] Batch {batch:4d} | Games: {games:5d} | Loss: {loss:.4f}\")\n",
    "else:\n",
    "    print(\"No logs yet. Start training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plot"
   },
   "source": [
    "## 7. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_metrics"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_file = LOG_DIR / 'training_log.jsonl'\n",
    "\n",
    "if log_file.exists():\n",
    "    with open(log_file) as f:\n",
    "        entries = [json.loads(line) for line in f]\n",
    "    \n",
    "    batches = [e['batch_num'] for e in entries]\n",
    "    losses = [e.get('loss', 0) for e in entries]\n",
    "    win_rates = [e.get('white_win_rate', 0) for e in entries]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(batches, losses, 'b-', alpha=0.7)\n",
    "    ax1.set_xlabel('Batch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Win rate\n",
    "    ax2.plot(batches, win_rates, 'g-', alpha=0.7)\n",
    "    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50%')\n",
    "    ax2.set_xlabel('Batch')\n",
    "    ax2.set_ylabel('Win Rate')\n",
    "    ax2.set_title('White Win Rate')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  Batches: {len(entries)}\")\n",
    "    print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"  Final win rate: {win_rates[-1]:.1%}\")\n",
    "else:\n",
    "    print(\"No logs found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test"
   },
   "source": [
    "## 8. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_test"
   },
   "outputs": [],
   "source": "import jax\nfrom flax.training import checkpoints\nfrom backgammon.training.train import create_train_state\nfrom backgammon.core.game import GameEngine\nfrom backgammon.evaluation.network_agent import NeuralAgent\nfrom backgammon.evaluation.agents import PipCountAgent\n\n# Load latest checkpoint\nprint(\"Loading latest checkpoint...\")\nrng = jax.random.PRNGKey(42)\nstate = create_train_state(config, rng)\nstate = checkpoints.restore_checkpoint(ckpt_dir=str(CHECKPOINT_DIR), target=state)\nprint(f\"‚úÖ Model loaded (step {int(state.step)})\")\n\n# Play test games: Neural vs Pip Count\nprint(\"\\nPlaying 10 test games: Neural vs Pip Count...\")\nneural = NeuralAgent(state=state, temperature=0.0, name=\"Neural\")\npip_agent = PipCountAgent(name=\"PipCount\")\n\nengine = GameEngine()\nwins = 0\nfor i in range(10):\n    result = engine.play_game(neural, pip_agent, seed=42 + i)\n    winner = \"Neural\" if result.winner == 1 else \"PipCount\"\n    wins += 1 if result.winner == 1 else 0\n    print(f\"  Game {i+1}: {winner} wins ({len(result.move_history)} moves)\")\n\nprint(f\"\\nNeural win rate: {wins}/10 ({wins*10}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": "---\n\n## Tips\n\n### Presets\n- **Quick** (`v6e_quick_training_config`): 2,500 games, small model, bfloat16. Good for validating the pipeline.\n- **Full**: 16,000 games, medium model. Real training.\n\n### Tuning\n- **Bigger model**: In full preset, increase `embed_dim=256`, `num_layers=6`, `ff_dim=1024`\n- **More games**: Increase `late_phase_games` (that's where the model improves most)\n- **No bfloat16**: Set `config.compute_dtype = None` (slower but higher precision)\n\n### Troubleshooting\n| Problem | Fix |\n|---------|-----|\n| No TPU detected | Runtime ‚Üí Change runtime type ‚Üí TPU |\n| OOM on GPU | Reduce `training_batch_size` to 128 |\n| Slow on CPU | Use the \"quick\" preset, or switch to TPU/GPU runtime |\n| Session timeout | Checkpoints are on Drive ‚Äî re-run cells 1-4, training resumes |\n\n### Resuming after disconnect\nCheckpoints are saved to Google Drive. Re-run cells 1-4 (install, verify, mount, config), then run cell 5. The `train()` function picks up from the latest checkpoint.\n\n---\n\n**Repo:** [github.com/wmhowell18/claude-code/transformer-backgammon](https://github.com/wmhowell18/claude-code/tree/main/transformer-backgammon)"
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Transformer Backgammon - TPU Training",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}