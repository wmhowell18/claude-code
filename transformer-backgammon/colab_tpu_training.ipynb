{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Transformer Backgammon - TPU Training on Google Colab\n",
    "\n",
    "This notebook trains a transformer-based backgammon AI on Google Colab TPUs.\n",
    "\n",
    "**Before running:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí TPU\n",
    "2. Make sure you have your code pushed to GitHub\n",
    "3. Update the `GITHUB_REPO` variable below with your repository URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup - Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_jax"
   },
   "outputs": [],
   "source": [
    "# Install TPU-specific JAX\n",
    "!pip install -q jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q flax optax\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_tpu"
   },
   "source": [
    "## 2. Verify TPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_tpu"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Force TPU backend\n",
    "jax.config.update('jax_platform_name', 'tpu')\n",
    "\n",
    "# Check devices\n",
    "devices = jax.devices()\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"Available devices: {devices}\")\n",
    "print(f\"Number of TPU cores: {len(devices)}\")\n",
    "\n",
    "# Quick test\n",
    "x = jnp.ones((1000, 1000))\n",
    "y = jnp.dot(x, x)\n",
    "print(f\"\\n‚úÖ TPU is working! Test computation result shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 3. Clone Your Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "git_clone"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# UPDATE THIS with your GitHub repository URL\n",
    "GITHUB_REPO = \"https://github.com/YOUR_USERNAME/transformer-backgammon.git\"\n",
    "BRANCH = \"claude/sprint2-encoder-mjosid5da1v9bhq8-EaMvJ\"  # Or your main branch\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('transformer-backgammon'):\n",
    "    !git clone {GITHUB_REPO}\n",
    "    %cd transformer-backgammon\n",
    "    !git checkout {BRANCH}\n",
    "else:\n",
    "    %cd transformer-backgammon\n",
    "    !git pull origin {BRANCH}\n",
    "\n",
    "# Install the package\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned and installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "## 4. Mount Google Drive (for saving checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_mount"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for this training run\n",
    "SAVE_DIR = Path('/content/drive/MyDrive/backgammon_training')\n",
    "CHECKPOINT_DIR = SAVE_DIR / 'checkpoints'\n",
    "LOG_DIR = SAVE_DIR / 'logs'\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"‚úÖ Logs will be saved to: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 5. Configure Training for TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": [
    "from backgammon.training.train import TrainingConfig\n",
    "\n",
    "# TPU-optimized configuration\n",
    "config = TrainingConfig(\n",
    "    # Training phases - adjust based on how long you want to train\n",
    "    warmstart_games=1000,      # Start with simple pip-count games\n",
    "    early_phase_games=5000,    # Neural self-play with simple variants\n",
    "    mid_phase_games=5000,      # Mixed complexity\n",
    "    late_phase_games=5000,     # Full complexity\n",
    "    \n",
    "    # Batch sizes - TPUs love large batches!\n",
    "    games_per_batch=50,         # Generate 50 games per batch\n",
    "    training_batch_size=256,    # Train on 256 positions at once (good for TPU)\n",
    "    \n",
    "    # Network architecture - using smaller default model\n",
    "    embed_dim=128,              # Embedding dimension\n",
    "    num_heads=8,                # Attention heads\n",
    "    num_layers=4,               # Transformer layers\n",
    "    ff_dim=512,                 # Feedforward dimension\n",
    "    dropout_rate=0.1,\n",
    "    \n",
    "    # Training mode\n",
    "    train_policy=True,          # Set to False for value-only training (simpler)\n",
    "    \n",
    "    # Replay buffer\n",
    "    replay_buffer_size=100000,  # Keep 100k positions in memory\n",
    "    replay_buffer_min_size=1000, # Start training after 1k positions\n",
    "    train_steps_per_game_batch=10, # Train 10 times per game batch\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=3e-4,\n",
    "    \n",
    "    # Self-play exploration\n",
    "    neural_agent_temperature=1.0,\n",
    "    \n",
    "    # Checkpointing and logging\n",
    "    checkpoint_every_n_batches=100,  # Save every 100 batches\n",
    "    log_every_n_batches=10,          # Log every 10 batches\n",
    "    checkpoint_dir=str(CHECKPOINT_DIR),\n",
    "    log_dir=str(LOG_DIR),\n",
    "    \n",
    "    # Random seed\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Total games: {config.warmstart_games + config.early_phase_games + config.mid_phase_games + config.late_phase_games}\")\n",
    "print(f\"  Batch size: {config.training_batch_size}\")\n",
    "print(f\"  Model size: {config.embed_dim}d, {config.num_layers} layers\")\n",
    "print(f\"  Training mode: {'Policy + Value' if config.train_policy else 'Value only'}\")\n",
    "print(f\"\\n‚úÖ Configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train"
   },
   "source": [
    "## 6. Run Training\n",
    "\n",
    "**Note:** This will take a while (several hours depending on configuration). The training will:\n",
    "1. Start with warmstart games (pip count vs pip count)\n",
    "2. Progress to neural self-play\n",
    "3. Save checkpoints to Google Drive every 100 batches\n",
    "4. Log metrics every 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "from backgammon.training.train import train\n",
    "\n",
    "# Run training!\n",
    "try:\n",
    "    train(config)\n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted. Checkpoints are saved in Google Drive.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## 7. Monitor Training Progress\n",
    "\n",
    "Run this cell in a separate tab while training is running to see real-time metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tail_logs"
   },
   "outputs": [],
   "source": [
    "# View the last 20 log entries\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "log_file = LOG_DIR / \"training_log.jsonl\"\n",
    "\n",
    "if log_file.exists():\n",
    "    with open(log_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"Total log entries: {len(lines)}\\n\")\n",
    "    print(\"Last 20 entries:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for line in lines[-20:]:\n",
    "        entry = json.loads(line)\n",
    "        phase = entry.get('phase', 'unknown')\n",
    "        batch = entry.get('batch_num', 0)\n",
    "        loss = entry.get('loss', 0.0)\n",
    "        games = entry.get('total_games', 0)\n",
    "        \n",
    "        print(f\"[{phase:8s}] Batch {batch:4d} | Games: {games:5d} | Loss: {loss:.4f}\")\n",
    "else:\n",
    "    print(\"No logs yet. Training hasn't started or just started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analyze"
   },
   "source": [
    "## 8. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_metrics"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "log_file = LOG_DIR / \"training_log.jsonl\"\n",
    "\n",
    "if log_file.exists():\n",
    "    # Load all log entries\n",
    "    with open(log_file, 'r') as f:\n",
    "        entries = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Extract metrics\n",
    "    batches = [e['batch_num'] for e in entries]\n",
    "    losses = [e.get('loss', 0) for e in entries]\n",
    "    win_rates = [e.get('white_win_rate', 0) for e in entries]\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(batches, losses, 'b-', alpha=0.7)\n",
    "    ax1.set_xlabel('Batch Number')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss Over Time')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Win rate plot\n",
    "    ax2.plot(batches, win_rates, 'g-', alpha=0.7)\n",
    "    ax2.set_xlabel('Batch Number')\n",
    "    ax2.set_ylabel('White Win Rate')\n",
    "    ax2.set_title('Win Rate Over Time')\n",
    "    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% (balanced)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Total batches: {len(entries)}\")\n",
    "    print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"  Final win rate: {win_rates[-1]:.2%}\")\n",
    "else:\n",
    "    print(\"No training logs found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_checkpoint"
   },
   "source": [
    "## 9. Load and Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax.training import checkpoints\n",
    "from backgammon.training.train import create_train_state\n",
    "from backgammon.core.game import GameEngine\n",
    "from backgammon.evaluation.network_agent import NeuralAgent\n",
    "from backgammon.evaluation.agents import PipCountAgent\n",
    "\n",
    "# Load the latest checkpoint\n",
    "print(\"Loading checkpoint...\")\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = create_train_state(config, rng)\n",
    "\n",
    "# Restore from checkpoint\n",
    "state = checkpoints.restore_checkpoint(\n",
    "    ckpt_dir=str(CHECKPOINT_DIR),\n",
    "    target=state,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# Play a test game\n",
    "print(\"\\nPlaying test game: Neural Network vs Pip Count Agent...\")\n",
    "\n",
    "neural_agent = NeuralAgent(\n",
    "    state=state,\n",
    "    temperature=0.0,  # Greedy selection for evaluation\n",
    "    name=\"NeuralNet\",\n",
    ")\n",
    "\n",
    "pip_agent = PipCountAgent(name=\"PipCount\")\n",
    "\n",
    "engine = GameEngine()\n",
    "result = engine.play_game(neural_agent, pip_agent, seed=42)\n",
    "\n",
    "print(f\"\\nüé≤ Game Result:\")\n",
    "print(f\"  Winner: {result.winner}\")\n",
    "print(f\"  Points: {result.points}\")\n",
    "print(f\"  Moves: {len(result.move_history)}\")\n",
    "print(f\"  Duration: {len(result.move_history)} turns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 10. Download Checkpoints (Optional)\n",
    "\n",
    "Your checkpoints are already saved in Google Drive, but you can also download them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_checkpoints"
   },
   "outputs": [],
   "source": [
    "# Create a zip file of checkpoints\n",
    "!cd /content/drive/MyDrive/backgammon_training && zip -r checkpoints.zip checkpoints/\n",
    "\n",
    "print(\"‚úÖ Checkpoints zipped!\")\n",
    "print(f\"   Location: {SAVE_DIR}/checkpoints.zip\")\n",
    "print(\"\\nYou can download this from your Google Drive or use the Colab file browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "---\n",
    "\n",
    "## Notes & Tips\n",
    "\n",
    "### Training Time Estimates\n",
    "- Warmstart (1000 games): ~30 min\n",
    "- Early phase (5000 games): ~3-4 hours\n",
    "- Mid phase (5000 games): ~3-4 hours\n",
    "- Late phase (5000 games): ~3-4 hours\n",
    "- **Total: ~10-13 hours for 16,000 games**\n",
    "\n",
    "### TPU Optimization Tips\n",
    "1. **Larger batches are better** - TPUs thrive on batch sizes of 128-512\n",
    "2. **Save often** - Colab sessions can timeout, checkpoint frequently\n",
    "3. **Use Google Drive** - Don't lose your work!\n",
    "4. **Monitor memory** - TPUs have limited memory, adjust if you see OOM errors\n",
    "\n",
    "### Adjusting Configuration\n",
    "- **Faster training**: Reduce `*_phase_games` values\n",
    "- **Better model**: Increase `embed_dim`, `num_layers`, or `ff_dim`\n",
    "- **Value-only mode**: Set `train_policy=False` for simpler training\n",
    "- **More exploration**: Increase `neural_agent_temperature` (e.g., 1.5)\n",
    "\n",
    "### Resuming Training\n",
    "If your session disconnects, just run cells 1-4 again, then modify cell 6:\n",
    "```python\n",
    "# Load existing checkpoint and continue\n",
    "state = checkpoints.restore_checkpoint(\n",
    "    ckpt_dir=str(CHECKPOINT_DIR),\n",
    "    target=state,\n",
    ")\n",
    "```\n",
    "\n",
    "### Common Issues\n",
    "- **TPU not available**: Make sure Runtime ‚Üí Change runtime type ‚Üí TPU\n",
    "- **Out of memory**: Reduce `training_batch_size` or `replay_buffer_size`\n",
    "- **Slow training**: Increase `training_batch_size` (TPUs like big batches)\n",
    "- **Session timeout**: Enable Colab Pro for longer sessions\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your training! üé≤ü§ñ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Transformer Backgammon TPU Training",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
